---
title: "Power Analysis of Microbiome Data"
author: "Michael Agronah"
#date: "2022-10-27"
date: '\today'
#date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  pdf_document:
    toc: true    # can remove
    toc_depth: 3 # can remove
    number_sections: true
  highlight: zenburn
header-includes:
- \usepackage{fancyhdr}
- \usepackage{mathtools}
- \usepackage{xcolor, hyperref}
- \usepackage{lipsum}
- \usepackage{caption}
- \usepackage{diagbox}
- \usepackage{multirow}
- \usepackage{amsmath}
- \setlength{\headheight}{28pt}
- \setlength{\footskip}{25pt}
- \pagestyle{fancy}
- \renewcommand{\headrulewidth}{0.5pt}
- \renewcommand{\footrulewidth}{0.5pt}
- \lhead{\includegraphics[width=8cm,height=1cm]{logo-McMaster}}
- \cfoot{School of Computational Science and Engineering \\ McMaster University}
- \rhead{\thepage}
- \hypersetup{colorlinks   = true, linkcolor=blue, urlcolor  = blue}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options:
  chunk_output_type: console
bibliography: packages.bib
---

\newpage 
<!-- # Overview -->
<!-- What is microbiome data -->
<!-- How it is presented  -->

<!-- # # Features of microbiome data -->
<!-- 16S rRNA gene sequencing and shotgun metagenomic sequencing are used to generate microbiome data [@xia2018statistical]. Microbiome data is frequently presented as OTUs or ASV in tables with rows as taxa by sample matrix. The information could be presented as counts or proportions (relative abundance). Figure .. shows a example of the data presented in terms of counts.  -->

<!-- Microbiome data contains a lot of zeros. It is spares so modeling such data is very difficult. The usual approach for transforming such data is the log ratio data, but the log rations cannot wpork in the presence of zeros so this makes it difficult and the usual statistical methods for modelling the data becomes challenging. The data is often highly dimensions with a complex covariant strucut and correlations sturcute. What that means is that you need some dimensional reduction methods to reduce the dimensionally to study and also compuationbaly challenges and numerical problems check this out as well. Microbiome data  also are over dispersied therfore you need a model that says dispetsion is more then amean sna dthe data ials has very disre read depths dso the usiual pison distributions cannot model such data adequratkey so you need som model that can do soevery effective job. HOw some perople have handles the over dispersion problem is to include a negative bionomial model and others have handles it by inclauding offset term to the problem and the zeros inflation or zero hurdle models have been used to handle the dispersion. Imputation methods have been used to handle compositionlaity and some other method of trandotaion like the arcsign and the bycentric method of trandofrmations have alsoe been applited. How used i use parallel in my work, I need to find out and also concerning the ime lines I would write the papaer, publishi it and then work on the Rage or go to the next papaer and then paublish that one too before I start working on the R codes of find our whcih of them works best whetherr I shsowuld be doing that concurently of at sepaerare tiems I will ask Ben and Mike for their taoughts. NOw we go to simulations and what I will do is that I will firts tes the efffectivennes of the pacjkaghes, simulations agian the simulated fold cahndge and make some stement abaout it, then I will ,model the the realdtions and then see whata is happening and see.   -->



<!-- *One potential answer to Paul's question about "what is a computational challenge" is simply that if you are using multi-sample computations like cross-validation, bootstrapping, tuning hyperparameters (i.e. computing optimal numbers of principal components) etc., on moderately large data set where fitting a single model takes a reasonable amount of time (say, 5 minutes), then you can often make use of distributed computing resources to handle the problem in a reasonable amount of time. (I don't consider this terribly *conceptually* deep, but some of -->
<!-- Paul's papers develop this approach)* -->

<!-- The ASV or OTU table is presented as a matrix with row denoting taxa and columns denoting or representing samples. Show figure to illustrate this. It can contain abundnace (counts) or relative abundances (proportions) -->

<!-- <!-- ### Microbiome Data Are Compositional --> -->
<!-- Microbiome data are compositional [@xia2018statistical]. The data even when presented as count describes only relative information. Each count describes a relative information of a whole. When the data  is presented as proportions, the data is then has  a sum constraints whihc introduces spurious correltion into the data and makes it difficult to be modelled by the usual statisctical methods for modeling data. The approaches used commonly for compositional data is to transform that into the Eucleadan space so standat approavhred can be used. and the usaall appraoch of trandoing the data is the log transofrmtion, how ever given that thet data hads a ot of zetros, the log ration teNformation cannot handle zeros so it becomes difficults. IN that case people hanve handles this by apply impuation meythosd to imnpute the zeos so that tah log tranfosmation methos can then be applied. Other shave come up qwith other methods wioch analayis the data in the cosntarin space that they are in whichout mapping thnem to the uscleadsian sapce. SSOm eot the tarnfoamtaional methosa are rthe bycetntric methicsdd used by .,.. and the ... .  -->


<!-- <!-- ### Microbiome Data Are High Dimensional and Underdetermined  --> -->
<!-- <!-- ### Microbiome Data Are Over-Dispersed  --> -->
<!-- <!-- ### Microbiome Data Are Often Sparse with Many Zeros  --> -->

<!-- ## Specific aims -->
<!-- Motivation for the projects. The motivation is where the knowlege on the literature ends.  -->

<!-- ### Power Analysis of Differential Abundance Studies -->

<!-- ### Longitudinal Microbiome studies -->


<!-- # Background -->



<!-- # Research Plan -->

## Power Analysis of Differential Abundance Studies

### Background 

Power analysis is a very important aspect of planning an effective study design. In microbiome analysis and  many control-case studies, an important question is to determine the minimum number of sample sizes needed to detect a desired effect size. Power analysis answers this question.  Power is the probability of correctly rejecting the null hypothesis in order to detect an effect size given that it really exist. Several factors influence power. Power depends on the sample size, effect size, the significant or alpha level as well as methodological factors such as experimental design, groups, statistical procedure and model, correlation between time points, response variable, and missing data. In situations where sample size and effect size are known, power analysis could be used to calculate the power of detecting the given effect size. 

The goal of differential abundance microbiome analysis is to test the null hypothesis that there are no differences between OTU abundances among  groups of interest. The standard procedure in hypothesis testing is to first define a null hypothesis and then choose a significant level. The test rejects the null hypothesis whenever $p$-value is below the significant level and fails to reject the null hypothesis whenever $p$-value is above the significant level. This technique has two potential drawbacks. First, rejecting the null hypothesis when the $p$-value is less than the significant level merely by chance. This results in what is known as the Type 1 error. The second drawback is failing to reject the null hypothesis when the $p$-value is greater than the significant level merely by accident. This is also known as Type 2 error. The goal of power analysis is to minimise these errors and to ensure that true effects are detecting with high probability.  

Differential microbiome analysis involves multiple hypothesis testing. Thus, there is the need for multiple hypothesis correction to correct for the high probability of detecting significant results from testing OTU individually. The standard statistical methods are used to adjust the multiple hypothesis correction (e.g., Benjamini-Hochberg method).

A number of factors poses difficulties when conducting power analysis for differential microbiome studies First, knowledge of the effect size is required to conduct a power analysis. A typical approach to obtain effect sizes is to use the effect sizes reported in the microbiome literature for power analysis. A problem in microbiome studies however, is that most publications do not report the effect sizes [@kers2021power]. Secondly, there is also the speculation of high publication biases in the field of microbiome studies, with work detecting low effects not being published and researches using approaches that may reveal large effects even when other methods show otherwise [@kers2021power].  @kers2021power and @kelly2015power, for instance, have shown that in studies that compare alpha and beta diversities between groups, the choice of the diversity index used determines the effect that will be observed. Consequently, researchers may vie towards choosing diversity measures that yield high effect sizes. These problems makes it difficult to judge the accuracy of those effect size reported in the literature and to use reported effect for performing power analysis. 

Thirdly, many differential abundance microbiome studies seen in the literature do not mention conducting power analysis prior to the studies [@kers2021power]. All these reasons has led to the believe that microbiome studies are underpowered with exagerated effect sizes [@kers2021power]. This believe may be justified by the lack of reproducibility experienced in the literature of microbiome studies [@kers2021power]. Early research on the relationship between the gut microbiota of obese and non-obesity groups for instance, found significant differences in the diversity of the gut microbiota and significant differences in the Bacteroidetes/Firmicutes (B/F) ratio between obese and non-obese people [@ley2006human]. Later research conducted using larger samples found only slight differences in microbial diversity and no statistically significant change in the B/F ratio between non-obese and obese individuals [@sze2016looking]. There is therefore a rising need to conduct power analysis in the field of microbiome studies [@kers2021power; @brussow2020problems]. 


In the literature of microbime studies, attention has been given to power calculations for microbiome studies comparing the diversity measures (for example, alpha, beta and gamma diversities) among groups of interest [@xia2018statistical]. Power calculations for such studies interested in comparing the diversities between 2 groups can be done in R using the **pwr.t.test** function in the pwr package or the **power.t.test** package in basic R. The package **pwr.avova.test** from the pwr package can be used for power analysis for studies involving more than two groups. The functions **power.prop.test** (statmod package), **pwr.chisq.test** (pwr package), **power.fisher.test** (statmod package) and **power.exact.test** (Exact package), can be used for power analysis for studies interested in comparing the abundances of a single taxon across groups. 

For studies involving diversity measures for the multivariate cases, @kelly2015power proposed a framework for PERMANOVA power estimation implemented in the **micropower** package in R and showed that the distance metric selected for a study has a significant impact on the observed effect. @kers2021power also performed power calculation to demonstrate how  different diversity metrics influence needed sample size in power calculation. 

Not much however, has been done about power analysis for studies interested in comparing the frequency of all taxa across groups of interest [@xia2018statistical]. A reparameterized Dirichlet multinomial model was developed by @la2012hypothesis to test the hypothesis of mean abundances as well as scales (variance comparison/dispersion) between groups. This paradigm is implemented in the **HMP** package in R. The package's capability includes the capacity to carry out parameter estimates, multiple hypothesis testing, and power and sample size calculations. In this package, the effect size is determined via a modified Cramer's criterion, which is a measure of the distance between the vector mean abundances between groups. The modified Cramer's criterion yields results in the range [0,1]. A criteria value of zero denotes no difference between the groups under study, whereas a value of one denotes the greatest difference possible.

Using simulation studies, @la2012hypothesis demonstrated that power is influenced by effect size and sample size, overdispersion, and, in some cases, sequencing depth. According to the studies, increasing the effect size and sample size increases power while increasing overdispersion decreases power. When effect size, overdispersion, and sample size are held constant in some of the examples in the studies, the number of reads had a positive effect on power. 
<!-- @hart2015navigating used the **HMP** package on a different dataset and reached the a similar conclusions regarding the relationships between power, effect size and overdispersion. -->

**HMP** provides point estimates for effect sizes and power and does not compute effect sizes and power for different ranges of abundances. A knowledge of the effect sizes and the power for different ranges of abundances, however, will provide researchers with an insight into the spectrum effect sizes and power to expect in a study design and be able to determine the average effect size and power across control abundances. Further, in a differential abundance study where the effect sizes and power to detect these effects for only specific taxa is of interest, knowing the effects and power for range of abundance will be handy. There is the need to develop a framework that does power and effect sizes calculations for different ranges of abundances. 

To the best of our knowledge, there currently exists no R packages that estimate the power and effects for various ranges of abundances. The Negative Binomial Model is commonly used in differential microbiome abundance studies [@xia2018statistical; @kodikara2022statistical], but to the best of our knowledge, there exists no R package for power analysis that is based on this model. In this project, we propose a framework for quantifying effect sizes and power for various control abundance based on the Negative Binomial Model. Two popular R packages that implement the Negative Binomial Model for differential abundance analysis are DESeq2 and edgeR [@xia2018statistical]. For simplicity, this study focuses on the implementation in the Deseq2 package. However, the same analysis can be repeated using the edgeR package as well. 

A key feature of our proposed method is that it can handle non-binary covariates. A limitation of both micropower and HMP is that they do not perform power calculation for studies involving non-binary covariates [@kelly2015power]. Both egdeR and DESeq2 allow the use use of continuous covariates [@robinson2010edger; @love2014moderated]. The methodology proposed in this project takes advantage of the this functionality implemented in egdeR and DESeq2 to calculate the power analysis for different abundance studies involving non-binary covariates.  The method proposed in this project will be implemented in R statistical package. Using this packages, researchers can conduct power analysis for differential abundance studies based on the Negative Binomial Model, estimate the number of taxa with high power and calculate the minimal number of sample size required to detect a specified range of effect sizes and power. 

### Proposed research
This research focuses on the power of differential microbiome studies that are based on the negative Binomial model. The specific goals of this project is to 

1. estimate effect sizes and power at various OTU abundances in microbiome differential abundance studies that are based on the Negative Binomial Models. 

2. estimate the expected number of OTUs in the study for which the null hypothesis can be rejected. 

3. estimate sample sizes needed to detect a given effect size and power

### Preliminary research
This section describes what I have done on this proposed research.

#### Data collection and processing:  

Using the search terms *"autism[All Fields] AND 16S[All Fields]"*, *"autism[All Fields] AND 16S[All Fields] AND Fecal[All Fields]"*, raw sequence data from 10 projects that examined the microbiome of children with autism spectrum disorder were gathered from the European Nucleotide Archive EBA and the National Center for Biotechnology Information NCBI. The following are the accession numbers for the 10 projects in the NCBI and ENA archives: PRJNA687773, PRJNA624252, PRJNA453621, PRJNA642975, PRJNA355023, PRJNA168470, PRJNA644763, PRJNA578223, PRJNA589343, and PRJEB45948. Children with autism spectrum disorders represent the treatment group in these datasets, whereas children with neurotypical or typical development are the control groups. Adaptor and primer sequences were removed using "cutadapt" function implemented in a bash script. The trimmed sequence from the "cutadpt" were then processed into Amplicon Sequence variants (ASVs) data using the Dada2 pineline which involves filtering and trimming, error estimation, denoising, merging paired reads and chimeras removal [@chen2020gut]. 

#### Models description: 
The null hypothesis ($H_0$) and the alternative hypothesis ($H_a$) for testing differences between 2 groups (control and treatment) is formulated by  $H_0: \mu_{control} - \mu_{treatment} = 0$ and $H_a: \mu_{control} - \mu_{treatment} \neq$ respectively, where $\mu_{control}$ and $\mu_{treatment}$ denote the means counts in control and treatment groups respectively. Effect sizes, a measure of the difference in the mean can then be tested.  The negative binomial distribution has often used to model microbiome count data due to the presence of overdispersion in microbiome count data. Let $K_ij$ denote the count data for the $i^{th}$ taxa in the $j^{th}$ sample.  $K_ij$  is modeled by  negative binomial distribution defined  by
\begin{align}
K_{ij} \sim NB(mean &= \mu_{ij}, dispersion = \alpha_i), \\
\mu_{ij} &= g^{-1}(E_{ij})\\ 
E_{ij} &= \sum_r x_{jr}\beta_{ir} ,
\end{align}
where $g$ is a link function, $\beta_{ir}$ are estimates of the effect sizes and $x_{jr}$ are the covariates. The relationship between the variance of counts and the dispersion is given by $Var K_{ij} = \mu + \alpha\mu^2$. In this project, the estimating procedure implemented in the *DESeq2* package in R is used for estimating $\beta_{ir},\mu_{ij}$ and $\alpha_i$. Details concerning this procedure is stated in the paper by @love2014moderated and by @anders2010differential. 

#### Modelling the relationship between control abundances and fold changes:
Plots of the control abundances and effect sizes are shown in figure \ref{cont_eff}. 
Variations around the smooth curve are greatest in the middle and lowest at the ends of the smooth curves. A scale-location plot shown in \ref{cont_eff} shows that variation exhibits as quadratic behaviour, confirming the relationship seen in the plot of control abundances fold changes \ref{cont_eff}. Thus, a quadratic function be used to describe the variance of the effect sizes as a function of control abundances. The variation in effect sizes is models by the scale parameter of a Cauchy distribution. The scale parameter is defined as a quadratic functions of the control abundances, shown in equation by \ref{eq1}. 
\begin{align} \label{eq1}
y_i &= Cauchy(location = 0, scale = \gamma_i), \gamma_i =  \exp(k_0 +  k_1x_i + k_2 {x_i}^2), 
\end{align}
where $y_i$ and $x_i$ are the effect size and control abundance, respectively, for the $i^{th}$ taxa and $k_j ; j = 1, 2, 3$ are constants to be estimated. 

```{r load_pkgs, include=FALSE, message=FALSE}
library(ggplot2)
library(fitdistrplus)
library(truncnorm)
library(BiocManager)
if (FALSE) BiocManager::install(c("DESeq2", "edgeR"))
library(nlme)
library(truncdist)
library(DESeq2)
library(tidyverse)
library(bbmle)
library(patchwork)
library(truncdist)
library(plyr)
library(metR)  
library(mgcv)
library(ggplot2)
library(patchwork)
library(extraDistr)
library(sn)
library(here)
setwd(here())
theme_set(theme_bw())
```


```{r load_data, include=FALSE, message=FALSE, echo=FALSE}
path <- "Power_Project/Datasets/"
plot_path <- "Latex_Files/Proposal/Figs/power/"
source("Power_Project/R_files/Functions.R")

plotname <- "box_plot_effects_"
#########################################
data_list <- mget(load("Power_Project/Datasets/data.RData"))
metadata_list <- mget(load("Power_Project/Datasets/groups.RData"))
name <- names(metadata_list); stopifnot(names(data_list) == name)
############################################################
subset <- c("PRJNA453621","PRJEB45948","PRJNA589343", "PRJNA687773")
names(subset) <- subset
data_list <- list(data_list[["PRJNA453621"]], data_list[["PRJEB45948"]],
                  data_list[["PRJNA589343"]], data_list[["PRJNA687773"]])

metadata_list <- list(metadata_list[["PRJNA453621"]],
                      metadata_list[["PRJEB45948"]],
                      metadata_list[["PRJNA589343"]],
                      metadata_list[["PRJNA687773"]])

names(data_list) <- names(subset)
names(metadata_list) <- names(subset)
name = names(subset)
######################################################################
DeseqRes <- mget(load(paste0(path,"DeseqResults.RData"))) #list of deseq results
deseq_list <- DeseqRes[[names(DeseqRes)]]    
stopifnot(names(deseq_list) == name)

Control_Treat <- mget(load(paste0(path,"Control_Treat_Data.RData")))  
Control_Treat <- Control_Treat[[names(Control_Treat)]]
stopifnot(names(Control_Treat) == name)

param.disp <- mget(load(paste0(path,"Dispersion_Parameters.RData")))
param.disp <- param.disp[[names(param.disp)]]
logcontrol <- read_data(Control_Treat, "LogControl")  
```


```{r control_effects, echo=FALSE, warning=FALSE, message=FALSE, out.width="50%",fig.align="center", fig.cap="\\label{cont_eff}  Plot of control against effect sizes for datasets with assession numbers PRJNA453621, PRJEB45948, PRJNA589343 and PRJNA687773", fig.height = 5}
plts=Plot_ContEffects(deseq_list,logcontrol, plot_path)
m = plts[[1]]; n = plts[[2]]
(m[[1]]|m[[2]])/(m[[3]]|m[[4]])
```

```{r scale_location plot, echo=FALSE, warning=FALSE, message=FALSE, out.width="50%",fig.align="center", fig.cap="\\label{cont_eff} Scale-location plot of control against squared fold changes for datasets with assession numbers PRJNA453621, PRJEB45948, PRJNA589343 and PRJNA687773", fig.height = 5}
(n[[1]]|n[[2]])/(n[[3]]|n[[4]]) #& theme(legend.position = "bottom")
#n + plot_layout(guides = "collect")
```

```{r control_fits, warning=FALSE, message=FALSE, echo=FALSE,out.width="50%",fig.align="center", fig.cap="\\label{fit_con} Histogram and density plots for control abundances and density from a simulated sample from fitting a truncated  normal distribution", fig.height = 5}
file_path <- "control_histograms_"
v1 = Fit_control(logcontrol,plot_path,file_path, plotname)
v1 = (v1[[1]]|v1[[2]])/(v1[[3]]|v1[[4]]) & theme(legend.position = "bottom")
v1 + plot_layout(guides = "collect")
```

```{r effect_fits, warning=FALSE, message=FALSE, echo=FALSE,out.width="50%",fig.align="center", fig.cap="\\label{fit_eff} Histogram and density plots for effect sizes", fig.height = 5}
param.control <- mget(load(paste0(path,"Control_Parameters.RData"))) 
param.control <- param.control[[names(param.control)]]

effects_list <- mget(load(paste0(path,"DeseqFoldChanges.RData")))
effects_list <- effects_list[[names(effects_list)]]

plotname  <- "effects_dist_"
v = Fit_Effects(effects_list,logcontrol,plot_path,plotname)
vv = (v[[1]]|v[[2]])/(v[[3]]|v[[4]]) & theme(legend.position = "bottom")
vv + plot_layout(guides = "collect")
```


```{r heatmap_plot,warning=FALSE, message=FALSE, echo=FALSE}
#################################################################
n_otu <- 300; n_samples <-  20; n_sim = 100
param.effects <- mget(load(paste0(path,"Effect_Parameters.RData")))
param.effects <- param.effects[[names(param.effects)]]
#data_sim(param.effects,param.disp,param.cont=param.control,n_sim,n_samples,n_otu,path,filter=10)
sim_data_list <- mget(load(paste0(path,"Simulated_Data.RData")))
sim_metadata_list <- mget(load(paste0(path,"Simulated_Metadata.RData")))
true_control_list <- mget(load(paste0(path,"True_Control.RData")))
true_effect_list <- mget(load(paste0(path,"True_Effect.RData")))
standard_err_delta_list <- mget(load(paste0(path,"Standard_Err_Delta.RData")))

standard_err_delta_list = standard_err_delta_list[[names(standard_err_delta_list)]]

sim_data_list = sim_data_list[[names(sim_data_list)]]
sim_metadata_list = sim_metadata_list[[names(sim_metadata_list)]]
true_control_list = true_control_list[[names(true_control_list)]]
true_effect_list = true_effect_list[[names(true_effect_list)]]
###############################################################
#Deseq1(sim_data_list,sim_metadata_list,path)
deseq_data_list <- mget(load(paste0(path,"Sim_Deseq_RESULT.RData")))
deseq_data_list <- deseq_data_list[[names(deseq_data_list)]]

#########################################################
combined_data =combined_data_list(true_control_list, true_effect_list,
                                 deseq_data_list,standard_err_delta_list, alpha=0.1)
r = Power_Heatmap(combined_data); plts1 = r[["full_plot"]]; plts2 = r[["reduced_plot2"]]

#b = (plts1[[1]]|plts1[[2]]) & theme(legend.position = "bottom")
#b + plot_layout(guides = "collect")

#b =(plts1[[3]]|plts1[[4]])  & theme(legend.position = "bottom")
#b + plot_layout(guides = "collect")
```


```{r contour, warning=FALSE, message=FALSE, echo=FALSE,fig.width=10, fig.height=8,fig.cap="\\label{fit_heat11} Contour plot showing ontrol abundances and effect sizes with power as contours"}
combined_data <- mget(load("Power_Project/Datasets/combined_data_list.RData"))
combined_data <- combined_data[[names(combined_data)]]
r = Power_Heatmap(combined_data)
plts1 = r[["full_plot"]]; plts2 = r[["reduced_plot2"]]

pr1 =contour_plots(combined_data, standardised = FALSE,
                                    interval=c(0.01,0.01,0.01,0.01))
                  #interval=c(0.005,0.0005,0.005,0.005))
pr2 =contour_plots(combined_data, deseqstandardised = TRUE,
                                    interval=c(0.01,0.01,0.01,0.01))
                  #interval=c(0.005,0.0005,0.005,0.005))

# n = (pr1[[1]][["contour_plot"]]|pr2[[1]][["contour_plot"]])/(pr1[[2]][["contour_plot"]]|pr2[[2]][["contour_plot"]]) & theme(legend.position = "bottom") 
#n + plot_layout(guides = "collect")
                                                                                                                       # /(pr1[[3]][["contour_plot"]]|pr2[[3]][["contour_plot"]])/(pr1[[4]][["contour_plot"]]|pr2[[4]][["contour_plot"]])  & theme(legend.position = "bottom")


#df =  data.frame(combined_data[[1]]$)
```



<!-- ```{r contour1, warning=FALSE, message=FALSE, echo=FALSE,fig.width=14, fig.height=15,fig.cap="\\label{fit_heat11} Heatmap showing the power and total number of taxa in grids of control and effect sizes"} -->
<!-- combined_data <- mget(load("Power_Project/Datasets/combined_data_list.RData")) -->
<!-- combined_data <- combined_data[[names(combined_data)]] -->
<!-- r = Power_Heatmap(combined_data) -->
<!-- plts1 = r[["full_plot"]]; plts2 = r[["reduced_plot2"]] -->

<!-- pr1 =contour_plots(combined_data, standardised = FALSE, -->
<!--                                     interval=c(0.01,0.01,0.01,0.01)) -->
<!--                   #interval=c(0.005,0.0005,0.005,0.005)) -->

<!-- pr2 =contour_plots(combined_data, deseqstandardised = TRUE, -->
<!--                                     interval=c(0.01,0.01,0.01,0.01)) -->
<!--                   #interval=c(0.005,0.0005,0.005,0.005)) -->

<!-- n = (pr1[[1]][["contour_plot"]]|pr2[[1]][["contour_plot"]])/(pr1[[2]][["contour_plot"]]|pr2[[2]][["contour_plot"]])/(pr1[[3]][["contour_plot"]]|pr2[[3]][["contour_plot"]])/ -->
<!--   (pr1[[4]][["contour_plot"]]|pr2[[4]][["contour_plot"]])  & theme(legend.position = "bottom") -->
<!-- n + plot_layout(guides = "collect") -->
<!-- ``` -->

\newpage
### Challenges
To perform power analysis, one must first determine the effect size and sample size. A common approach is to obtain the effect size reported in the literature to conduct power analysis and sample size calculation. Most differential abundance microbiome studies in the literature, however, do not report the effect sizes [@kers2021power].  There is also the speculation of publication biases with many microbiome studies reporting only results that produce high effects even with low sample sizes [@kers2021power]. Further, microbiome studies are known to be underpowered  [@kers2021power; @yap2021autism].  Additionally, reported effects in the literature do not report effect sizes and power for different OTU abundances. Knowing the effect sizes for different OTU abundances would help researchers to estimate the range of effects and power across various OTU abundances and not lump all OTUs as having the same effect sizes or power. 

There is therefore the need for a framework that estimates effect sizes for differential abundance microbiome studies for performing power analysis and sample size calculation. 

This project proposes a framework for estimating the effect sizes and power for different ranges of OTU abundances in a differential abundance microbiome studies. The method proposed in this project is based on the Negative Binomial Model. To the best of our knowledge, there exists no current work or R package that estimates power and effect sizes for various OTU abundances in differential abundance studies.   

<!-- Also, the 2 most popular packages on power calculation do not allow for non binary covariance. Taking advantage of this project is that we take advantage of the framework in deseq which allows for non-binary covariate to calculate power for differential abundance analysis where one is interested in looking that the effects of non-binary covariate as well. Give an example of where  this might be of interest.  -->


### Project Contributions:

This project advances the field of differential abundance microbiome research. Based on the Negative Binomial model, we propose a method for estimating the effect sizes and power of individual OTU abundances. The method described here will be implemented in R statistical packages. Using this framework, researchers will be able to tell the spectrum of effect sizes and power for different OTU abundances, gaining insight into how effect sizes and power are distributed across different OTU abundances. In applications where researchers are interested in differential abundance of specific OTUs, for instance, they will be able to estimate the effect size and power of the desired OTUs using the proposed framework in this project. Researchers will also be able to predict the number OTUs that have high power.   

A special feature of the method  proposed in this project is that it allows for the use of non-binary covariates. This approach, for instance, can be used to determine how a continuous covariate affects the differential abundance of OTUs. Two widely used R packages for power calculation; HMP and Micropower, do not support non-binary covariates.  Our approach makes use of the DEseq2 framework, which supports non-binary covariate, to calculate power and effect size.



### Conclusion

<!-- A line may be enough -->
<!-- ```{r} -->
<!-- mm <- mle2(logfoldchange ~ dt(df=a, ncp=b), -->
<!--             start =list(a=10, b=1), data=df) -->
<!-- mm   = rt(1000, df=100.769161805, ncp =  0.003052541) -->
<!--   hist(Lfoldchange,probability = T) -->
<!--   lines(density(Lfoldchange)) -->

<!-- mm<-mle2(logfoldchange ~ dcauchy(0, scale=exp(a+b*logcontrol + c*logcontrol^2)), -->
<!--              start =list(a=0.1, b=0.1, c= 0.001), data=df) -->

<!-- mm = mle2(dt(x, df, ncp, log = FALSE)) -->

<!-- rt(n=1000, df=2, ncp=4) -->

<!-- ``` -->




<!-- 1. Overview and background of the project -->
<!-- 2. What are the research questions are you addressing? -->
<!-- 3. What are the challenges in addressing these questions? -->
<!-- 3. What have people done about the problem already. What methods exist. How are these methods being applied both within the field of microbiome studies and as well as in other related fields. describe these methods. -->
<!-- 4. What packages exist. How do the packages work. What are the limitations and advantages of each of those packages. How is yours different? -->
<!-- 5. How would l address that challenges I have mentioned -->
<!-- 6. What is my contribution to the research area. What make your method different. What are some of the existing packages for the things you are doing. What are some of the loop holes in the current literature. How do you addres them and what is unquie about your method. -->
<!-- 7. What have you already done about the problem: simulation studies -->

<!-- # Research Plan -->

<!-- ## The procedure -->
<!-- Write a clear distribution of the procedure. -->


<!-- ## Chapter 1 Power analysis -->
<!-- ### Overview and background -->

<!-- #### Factors that influences power of a study -->

<!-- Power is a function of effect sizes, alpha, and sample sizes. Power,effect sizes, alpha, and sample sizes are all related. Each is dependent on the other three (Cohen 1988). (page 14). For example, sample sizes can be calculated given effect size, power, and alpha level, and effect sizes can be calculated given power, sample size, and alpha level. Other factors affecting power are the experimental design, number of groups, statistical procedure and model, correlation between time points, response variable, and missing data (Xia, ..). For example: … -->

<!-- #### Multiple hypothesis testing -->

<!-- 1. Include timeline in your work. -->
<!-- Include pages -->
<!-- WhatI will do shou -->


<!-- 1. Expounding on the research questions 20 pages -->
<!-- The challenges -->

<!-- ### Research questions -->
<!-- 1. investigate the relationships among power, effect sizes and abundances with the aim of understanding the mechanisms behind these relationships. -->
<!-- 2. investigate the number of sample sizes required to detect a given power and a given effect size. -->

<!-- ### Literature review -->

<!-- 1. Other areas the methods are being applied -->

<!-- #### Our Procudure -->
<!-- We might need to specify a range of distributions in the package for that best fit the control and do a goodness of fit test to see which works best because each dataset differs.  a given dataset because all the datasets differ. This -->

<!-- We might need to specify a few different distrbutions based on what we have seen that the distributions of controls for different datasets look like ti decide whcih of these distributions best fit a given  dataset. -->
<!-- We get the dataset, we fit the dataset -->

<!-- You need to find out the effects reported in the datasets you are using and compare them to the effect you are getting. -->

<!-- ### The delta method -->
<!-- Write up on the delta method. The delta method will be used to estimate the standardised error for the control abundance and the effect sizes defined -->

<!-- The delta method is a method for claucluationg the  assymptotic bal bal bale. -->
<!-- For a random variable X and parameter $\theta, \hat{\theta}$ and any smooth function f   with the property that $f'(\theta)$ exists and is non-zero valued, the delta method states that -->
<!-- $$ -->
<!-- \frac{\hat{\theta} - \theta}{\hat{SE}_\theta} \xrightarrow{} N(0,1),\,\, then \\ -->
<!-- \frac{f(\hat{\theta}) - f(\theta)}{f'(\theta) \hat{SE}_\theta} \xrightarrow{} N(0,1) -->
<!-- $$ -->
<!-- and tha asymptotic mean and asymptotic standard error of $f(\hat{\theta})$ is $f(\theta)$ and $f('{\hat{\theta}}) \hat{SE}(\theta)$ respectively. -->


<!-- Let $C$ and $T$ denote the control and treatment groups respectively. The standard error of  fold changes is define be estimated as follows: -->

<!-- $$\textbf{var}[\log_2\left(\frac{T}{C}\right)] = \textbf{var}[\log_2(T)] + \textbf{var}[\log_2(C)]$$ -->
<!-- From Taylor series expansion $\textbf{var}[f(X)]  = \textbf{var}[f(\mu_X + X - \mu_X)]  \approx \textbf{var}[f(\mu_X) + f'(\mu_X)(X - \mu_X) + \frac{f''(\mu_X)(X - \mu_X)]}{2} + ..= f'(\mathbb{E}[X])\textbf{var}[X]^2$ -->
<!-- , -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- for f(X) = \log_2(X), f' &= \frac{1}{x\ln 2}\\ -->
<!-- \textbf{var}[\log_2\left(\frac{T}{C}\right)] &= \textbf{var}[\log_2(T)] + \textbf{var}[\log_2(C)]\\ -->
<!-- &\approx \frac{1}{T\ln 2}\textbf{var}[T]    +  \frac{1}{C\ln 2}\textbf{var}[C]\\ -->
<!-- \mathbb{SE}[\log_2\left(\frac{T}{C}\right)] &= \sqrt{\frac{1}{T\ln 2}\textbf{var}[T]    +  \frac{1}{C\ln 2}\textbf{var}[C]} -->
<!-- \end{aligned} -->
<!-- $$ -->





<!-- #### Difference between what we seek to do and HMP -->

<!-- #### The limitations of the multinomial modeling in modeling microbiome data -->

<!-- The multinomial model assumes that the true frequency of each taxon is the same for all samples. This then implies that as the number of reads within each sample increases, taxa frequencies in all samples converge to the same value. Example 40% taxa A, 20% taxa B, etc.. for all samples. When there is overdispersion, that convergence does not work Thus it is incapable of handling overdispersion in the data and will result in type 1 error if it is used for hypothesis testing. -->

<!-- #### The Dirichlet-multinomial distribution -->

<!-- 1. It has 2 sets of parameters $pi$;a vector of relative abundances of taxa j the proportion of  and $\theta$ -->
<!-- 2. How does it do the modelling -->
<!-- 3. How is the parameter estimation done? -->

<!-- So estimation of the model parameters is done by the method of moment estimates of mle. Testing pf the hypothesis is done by the generalised wald test. -->

<!-- *It was proposed to test the null hypothesis using the generalized Wald test of Koehler and Wilson (1986) and La Rosa et al. (2012).* -->

<!-- ### Challenges -->
<!-- ### Current work -->

<!-- #### Simulations studies -->
<!-- ### References -->

<!-- ## Chapter 2 Longitudinal studies -->
<!-- THe main idea is -->
<!-- Where are some of thte methods being used in longitudianl data analysis? Hwo are these methods being used? What packages are there, what kinds of reseacrh questions does longituidanal data analysis answer? WHat can be done. What are some of the challenges. -->
<!-- Get the general picture -->


<!-- key scientific questions investigators are addressing with longitudinal microbiome analyses. -->



<!-- The questions that longitudinal microbiome studies often address in the literature are as follows. MOdelling the ... These mgroupss of methods have been used often in the literature -->

<!-- #### gllvm -->
<!-- “Jenni Niku” ([Niku et al., 2017, p. 498] extended  the generalized linear model to include a latent variable to account for covariation between species and correlation and applied it to the model metagenomic dataset. This  generalized linear latent variable model is implemented in the gllvm package in R and can be used for fitting longitudinal data. -->

<!-- The algorithm used in the model-fitting is fast and can handle much larger datasets ()MLE and avoids MC estimations. Thus it can handle large datasets relatively quicker than other existing packages. Secondly, it uses a maximum likelihood framework, which allows the use of likelihood-based tools for inferencing.  A limitation however, of gllvm is that it does assume observations are independent and thus does not account for correlation. Also, it does not include random slopes for predictors, which could enhance the account for the interspective variations in the responses. -->

<!-- ### Research Questions -->

<!-- The goal of this project is to -->
<!-- 1. model the relationship between otu counts and covariates (examples; time, age, gender and groups) -->
<!-- while capturing the correlation structure within and between individual taxa and temporal patterns of -->
<!-- taxa over time. -->
<!-- 2. find taxa with differential abundances over time between groups. We consider a simple of where there -->
<!-- are only 2 groups but the model can be extended to more than two groups. Model how microbiome -->
<!-- abundance changes over time between groups. -->




<!-- ## Chapter 3 Handling compositionality in microbiome data -->
<!-- One has said that you can avoid compositionality by sequesnsiyng using.... However, itis still a problem -->

<!-- ## What are the cahllenges of each research question in each chapter -->


<!-- 1. Understand false discovery rates and its relations to effect sizes in general. -->
<!-- Understand what the literature says concerning false discovery rate and effect sizes -->





<!-- # Literature Review -->

<!-- # Research Plan -->

<!-- Notes -->
<!-- Type 1 error rate. -->
<!-- The probability of the type I error (a true null hypothesis is rejected) is commonly called the significance level of the hypothesis test and is denoted by -->


<!-- It was proposed to test the null hypothesis using the generalized Wald test of Koehler and Wilson (1986) and La Rosa et al. (2012). -->


<!-- Packages used for differential gene expressions -->

<!-- 1. Deseq2, edgeR, limma, -->

<!-- ## Cpater for will be to handle compositionallity -->
